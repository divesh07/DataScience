{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standard Scalar:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[0, 150],[1, -10]]\n",
    "# scale data according to computed scaling values\n",
    "StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "[[ 1.26398112 -1.16389967]\n",
      " [-1.06174414  0.12639634]\n",
      " [ 0.         -1.05856939]\n",
      " [ 0.96062565  0.65304778]\n",
      " [-1.16286263  1.44302493]]\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    "\t\t\t\t[8, 0.05],\n",
    "\t\t\t\t[50, 0.005],\n",
    "\t\t\t\t[88, 0.07],\n",
    "\t\t\t\t[4, 0.1]])\n",
    "print(data)\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2204460492503132e-17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "[[1.         0.        ]\n",
      " [0.04166667 0.49494949]\n",
      " [0.47916667 0.04040404]\n",
      " [0.875      0.6969697 ]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Minmax scalar:\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    "\t\t\t\t[8, 0.05],\n",
    "\t\t\t\t[50, 0.005],\n",
    "\t\t\t\t[88, 0.07],\n",
    "\t\t\t\t[4, 0.1]])\n",
    "print(data)\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.max(scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# create a pipeline object\n",
    "pipe = make_pipeline(StandardScaler(), GaussianNB())\n",
    "# load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# fit the whole pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "#Pipeline(steps=[('standardscaler', StandardScaler()),('logisticregression', LogisticRegression())])\n",
    "# we can now use it like any other estimator\n",
    "accuracy_score(pipe.predict(X_test), y_test)\n",
    "print(pipe.predict(X_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 16,  0],\n",
       "       [ 0,  0,  9]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion Matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr=classification_report(y_test, pipe.predict(X_test))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
