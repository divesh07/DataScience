{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Text fg Analytics is boring boring boring!!\"\n",
    "doc2 = \"Analytics is interesting\"\n",
    "doc3 = \"We want interesting sports analytics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(sent):\n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    stop_updated = stopwords.words(\"english\") + list(punctuation) \n",
    "    final_word = [term for term in tokens if term not in stop_updated \n",
    "               and len(term) > 2] \n",
    "    res = \" \".join(final_word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_clean = clean_txt(doc1)\n",
    "doc1_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2_clean = clean_txt(doc2)\n",
    "doc2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3_clean = clean_txt(doc3)\n",
    "doc3_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.DataFrame([doc1_clean, doc2_clean, doc3_clean], columns=[\"text\"])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer of sklearn gives us the DTM\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range =(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking fit_transoform on the CountVectorizer object\n",
    "X = count_vect.fit_transform(doc['text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fit -> extract distinct words to form a bag of words\n",
    "- transform -> compute the occourrence of every word from the BOW in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the BOW's\n",
    "bow = count_vect.get_feature_names()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the position of every word in the BOW, as a dictionary\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the position of a particular word in BOW\n",
    "count_vect.vocabulary_['boring boring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the document term matrix can be done using toarray() or todense()\n",
    "#methods\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM = pd.DataFrame(X.toarray(),columns = count_vect.get_feature_names())\n",
    "DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM = pd.DataFrame(X.toarray(),columns = bow)\n",
    "DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term document matrix or TDM is a transpose of DTM \n",
    "#which is used in finding similarity between words\n",
    "TDM = DTM.T\n",
    "TDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a newly seen document using the same DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndoc = [\"awesome paper is not always boring boring\",\"is paper really text analytics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.transform(ndoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTMnd = pd.DataFrame(count_vect.transform(ndoc).toarray(),columns = count_vect.get_feature_names())\n",
    "DTMnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram analysis\n",
    "- unigram -> a token comprises of a exactly a single word\n",
    "- bigram -> a token comprises of exactly two words\n",
    "- trigram -> a token comprises of eaxctly three words\n",
    "\n",
    "### The next immediate word(s) are grouped with the current word to form a bigram or a tri-gram\n",
    "- For example: suppose we have a string \" how are you doing\"\n",
    "- unigram :{\"how\", \"are\", \"you\", \"doing\"}\n",
    "- bigram: {\"how are\", \"are you\", \"you doing\"}\n",
    "- trigram:{\"how are you\", \"are you doing\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any combination of unigrams, bigrams and trigrams can be obtained using a Countvectorizer\n",
    "\n",
    "- By default the vectorizer's in sklearn will perform on unigrams, i.e ngram_range will be set to (1,1)\n",
    "- to extarct bigrams ngram_range = (2,2)\n",
    "- to extract trigrams ngram_range = (3,3)\n",
    "- To extract both unigrams and bigrams ngram_range = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A token is called as a bigram if its min and max length is 2\n",
    "count_vect_bg = CountVectorizer(ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bg = count_vect_bg.fit_transform(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect_bg.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM_bg = pd.DataFrame(X_bg.toarray(),columns=count_vect_bg.get_feature_names())\n",
    "DTM_bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM_bg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_ubg = CountVectorizer(ngram_range=(1,1),max_features = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ubg = count_vect_ubg.fit_transform(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect_ubg.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM_ubg = pd.DataFrame(X_ubg.toarray(),\n",
    "                      columns=count_vect_ubg.get_feature_names())\n",
    "DTM_ubg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Setting max features\n",
    " \n",
    " 1. your vectorizer's take another argument called max_features where in you can specify the top n features to be selected\n",
    " 2. on your DTM, if you perfrom column sum, you will get the total No. of occourrence of a word accross all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top 3 features in our DTM based on their frequencies\n",
    "#accross all the documents in the corpus by taking a column sum of all the words in the DTM\n",
    "count_nfeatures = CountVectorizer(max_features=6,ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nfeatures = count_nfeatures.fit_transform(doc['text'])\n",
    "doc['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nfeatures.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DTM_nfeatures = pd.DataFrame(X_nfeatures.toarray(),\n",
    "                            columns=count_nfeatures.get_feature_names())\n",
    "DTM_nfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DTM for all the trigrams - CountVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,1),max_features = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_vect.fit_transform(doc['text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(X.toarray(), columns=tfidf_vect.get_feature_names())\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_square = tfidf.apply(lambda x: x**2)\n",
    "tfidf_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_square.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Spotting common non-dictionary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revlst = [\"this\",\"is\",\"update\",\"good\",\"lyf\",\"life\",\"bdfgbvd\",\"zebra\",\"lyf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndwords = [word for word in revlst if word not in word_list]\n",
    "ndwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) REgular Expression - some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = \"IIT Ropar is best Ropar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsent = re.sub(\"Ropar\",\"Roorkee\",sentence)\n",
    "newsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"IIT Ropar., is !@ best Ropar?? #$ 12345\"\n",
    "newsent = re.sub(r\"[^\\w\\s]+\",\" \",sentence)\n",
    "newsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
