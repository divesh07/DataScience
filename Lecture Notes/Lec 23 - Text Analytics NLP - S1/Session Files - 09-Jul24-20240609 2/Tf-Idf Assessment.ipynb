{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0386e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assign documents\n",
    "d0 = 'the quick brown fox jumps over the lazy dog!!!'\n",
    "d1 = 'the lazy dog sleeps in the warm sunlight.'\n",
    "d2 = 'the quick brown fox is very agile and clever??'\n",
    "d3 = 'the sunlight, makes the lazy dog feel warm.'\n",
    "d4 = 'the agile fox jumps over the brown dog.'\n",
    "\n",
    "\n",
    "def clean_txt(sent):\n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    stop_updated = list(punctuation)\n",
    "    final_word = [term for term in tokens if term not in stop_updated \n",
    "               and len(term) > 2]\n",
    "    res = \" \".join(final_word)\n",
    "    return res\n",
    "\n",
    "# Function to remove punctuation\n",
    "#def remove_punctuation(text):\n",
    "#    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Remove punctuation\n",
    "# d0 = remove_punctuation(d0)\n",
    "# d1 = remove_punctuation(d1)\n",
    "# d2 = remove_punctuation(d2)\n",
    "# d3 = remove_punctuation(d3)\n",
    "# d4 = remove_punctuation(d4)\n",
    "\n",
    "d0 = clean_txt(d0)\n",
    "d1 = clean_txt(d1)\n",
    "d2 = clean_txt(d2)\n",
    "d3 = clean_txt(d3)\n",
    "d4 = clean_txt(d4)\n",
    "\n",
    "# Merge documents into a single corpus\n",
    "string = [d0, d1, d2, d3, d4]\n",
    "\n",
    "# Define custom stop words\n",
    "stop_words = [\"the\", \"over\", \"in\", \"fox\", \"dog\", \"very\", \"and\", \"is\"]\n",
    "\n",
    "# Create TF-IDF vectorizer object with custom stop words\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a56c1095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF value for 'quick' in d0: 0.5440812430630017\n"
     ]
    }
   ],
   "source": [
    "# Get TF-IDF values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get TF-IDF values for \"quick\" in d0\n",
    "index_quick = np.where(feature_names == 'quick')[0][0]\n",
    "tfidf_values = result.toarray()\n",
    "tfidf_quick_d0 = tfidf_values[0][index_quick]\n",
    "\n",
    "print(f\"TF-IDF value for 'quick' in d0: {tfidf_quick_d0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16403c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word with the highest TF-IDF value in Sentence 2: 'sleeps' with a value of 0.6030\n"
     ]
    }
   ],
   "source": [
    "# Get TF-IDF values for Sentence 2\n",
    "tfidf_values = result.toarray()\n",
    "tfidf_sentence_2 = tfidf_values[1]  # Sentence 2 is at index 1\n",
    "\n",
    "# Find the word with the highest TF-IDF value in Sentence 2\n",
    "max_tfidf_index = np.argmax(tfidf_sentence_2)\n",
    "max_tfidf_word = feature_names[max_tfidf_index]\n",
    "max_tfidf_value = tfidf_sentence_2[max_tfidf_index]\n",
    "\n",
    "print(f\"Word with the highest TF-IDF value in Sentence 2: '{max_tfidf_word}' with a value of {max_tfidf_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4da22a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF value for 'lazy' in Sentence 4: 0.3458\n"
     ]
    }
   ],
   "source": [
    "# Get TF-IDF values for Sentence 4\n",
    "tfidf_values = result.toarray()\n",
    "tfidf_sentence_4 = tfidf_values[3]  # Sentence 4 is at index 3\n",
    "\n",
    "# Get the TF-IDF value for 'lazy' in Sentence 4\n",
    "index_lazy = np.where(feature_names == 'lazy')[0][0]\n",
    "tfidf_lazy_d4 = tfidf_sentence_4[index_lazy]\n",
    "\n",
    "print(f\"TF-IDF value for 'lazy' in Sentence 4: {tfidf_lazy_d4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87c0b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with the highest TF-IDF value for 'sunlight': 'the lazy dog sleeps the warm sunlight'\n",
      "TF-IDF value: 0.4865\n"
     ]
    }
   ],
   "source": [
    "# Get TF-IDF values for each sentence\n",
    "tfidf_values = result.toarray()\n",
    "\n",
    "# Find the index of the word 'sunlight'\n",
    "index_sunlight = np.where(feature_names == 'sunlight')[0][0]\n",
    "\n",
    "# Extract TF-IDF values for 'sunlight' in all sentences\n",
    "tfidf_sunlight = tfidf_values[:, index_sunlight]\n",
    "\n",
    "# Find the sentence with the highest TF-IDF value for 'sunlight'\n",
    "max_tfidf_index = np.argmax(tfidf_sunlight)\n",
    "max_tfidf_sentence = string[max_tfidf_index]\n",
    "max_tfidf_value = tfidf_sunlight[max_tfidf_index]\n",
    "\n",
    "print(f\"Sentence with the highest TF-IDF value for 'sunlight': '{max_tfidf_sentence}'\")\n",
    "print(f\"TF-IDF value: {max_tfidf_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97abdc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF value for 'brown' in Sentence 5: 0.5062\n"
     ]
    }
   ],
   "source": [
    "# Get TF-IDF values for Sentence 5\n",
    "tfidf_values = result.toarray()\n",
    "tfidf_sentence_5 = tfidf_values[4]  # Sentence 5 is at index 4\n",
    "\n",
    "# Get the TF-IDF value for 'brown' in Sentence 5\n",
    "index_brown = np.where(feature_names == 'brown')[0][0]\n",
    "tfidf_brown_d5 = tfidf_sentence_5[index_brown]\n",
    "\n",
    "print(f\"TF-IDF value for 'brown' in Sentence 5: {tfidf_brown_d5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d91426f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features (unigrams + bigrams): 25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the sentences\n",
    "documents = [\n",
    "    'the quick brown fox jumps over the lazy dog',\n",
    "    'the lazy dog sleeps in the warm sunlight',\n",
    "    'the quick brown fox is very agile and clever',\n",
    "    'the sunlight makes the lazy dog feel warm',\n",
    "    'the agile fox jumps over the brown dog'\n",
    "]\n",
    "\n",
    "# Define stop words\n",
    "stop_words = {'the', 'over', 'in', 'fox', 'dog', 'very', 'and', 'is'}\n",
    "\n",
    "def preprocess(document):\n",
    "    # Remove punctuation and lowercase words\n",
    "    words = document.lower().replace('.', '').replace(',', '').replace('!', '').replace('?', '').split()\n",
    "    # Remove stop words\n",
    "    return ' '.join(word for word in words if word not in stop_words)\n",
    "\n",
    "# Preprocess documents\n",
    "cleaned_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Create a CountVectorizer object for unigrams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # (1, 2) means unigrams and bigrams\n",
    "\n",
    "# Fit and transform the cleaned documents\n",
    "count_matrix = vectorizer.fit_transform(cleaned_documents)\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Count the total number of features\n",
    "total_features = len(feature_names)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of features (unigrams + bigrams): {total_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "580414ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    'the quick brown fox jumps over the lazy dog',\n",
    "    'the lazy dog sleeps in the warm sunlight',\n",
    "    'the quick brown fox is very agile and clever',\n",
    "    'the sunlight makes the lazy dog feel warm',\n",
    "    'the agile fox jumps over the brown dog'\n",
    "]\n",
    "\n",
    "# Define stop words\n",
    "stop_words = {'the', 'over', 'in', 'fox', 'dog', 'very', 'and', 'is'}\n",
    "\n",
    "def preprocess(document):\n",
    "    # Remove punctuation and lowercase words\n",
    "    words = document.lower().replace('.', '').replace(',', '').replace('!', '').replace('?', '').split()\n",
    "    # Remove stop words\n",
    "    return ' '.join(word for word in words if word not in stop_words)\n",
    "\n",
    "# Preprocess documents\n",
    "cleaned_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer with both unigrams and bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Number of features\n",
    "num_features = len(features)\n",
    "print(f\"Total number of features: {num_features}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
